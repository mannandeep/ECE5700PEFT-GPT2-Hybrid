{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk1yykXrnw7n"
   },
   "source": [
    "```\n",
    "‚öôÔ∏è Step 1: Check runtime hardware (GPU & RAM)\n",
    "\n",
    "This cell shows your Colab runtime setup, whether a GPU is available and how much RAM is allocated.\n",
    "\n",
    "üü© This project was trained using Google Colab Pro with GPU acceleration (T4 or A100).  \n",
    "üí° To re-run this notebook yourself:\n",
    "\n",
    "1. Go to Runtime > Change runtime type\n",
    "2. Set Hardware Accelerator to GPU\n",
    "3. (Optional) Select High-RAM if available\n",
    "\n",
    "These settings significantly reduce training time and memory issues.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxMyX--EG6eK",
    "outputId": "5d380196-f433-4480-b5bc-74fa049ca3b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 13 16:03:09 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   31C    P0             46W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAKsS-OeG7v4",
    "outputId": "35fc7dd6-6a49-4986-cceb-c9b38af4132b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM available: 89.6 GB\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print(f\"RAM available: {ram_gb:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iv3ExkO_prwz"
   },
   "source": [
    "```\n",
    "‚öôÔ∏è Step 2: Install required Python packages\n",
    "\n",
    "Installs Hugging Face's `transformers`, `datasets`, and `tqdm`. These are required for model loading, dataset handling, and progress bars.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cj5Ydik5s8Zi",
    "outputId": "de54bea4-6da7-4f92-ed8f-1f41df34c0f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vN1kihuMp1Gm"
   },
   "source": [
    "```\n",
    "üì• Step 3: Import all dependencies\n",
    "\n",
    "This includes PyTorch, Hugging Face libraries, AMP (for faster training), and other utilities needed for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dlKHYP2jtAGh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "7d2fbb3081cc484cb168c7fdb3f93b9e",
      "de939d0db4bf400c885ed6d45c44e1fe",
      "2ca157c3446945478922d4d0cc4768f0",
      "45c6cc0d8d0246df979a8bbb6fa0395d",
      "d5092ef5b44848869280f71279717640",
      "185f40c833ec4b6eb88e7e9871796327",
      "c395f686de9c4eab8208bf6e4d34598b",
      "181b280ee2c746b4bb6bd7ed740c4418",
      "437acc92f9074f8ebd743a5dbb8303db",
      "e10c61f48f0e498c81432416664f475f",
      "300db752ea0241ed8372cd27235569fc",
      "461c804808724453a31425a8c53a66b3",
      "d63db77ec7a842e88194f595bdddb11d",
      "a6b0cabdc1c64e9d8395516297465ff5",
      "a0ca41c30d96454890bcd0a4f9ac595b",
      "aaa53bbc109e42dd97fe28027507a533",
      "5af717af54f440ecbc31a5b915c5666a",
      "d9bca9755ea443378e2b4f6bcd4db86c",
      "1dc575c5c8df495589a1ba546969ced3",
      "a861bc58a7ce4ffba5ceb3e1e7497897",
      "b8494032b25b42c1b6c799d9fe8bfbcd",
      "f9515e7205e840f994cc189513634d8a",
      "33eab9bfd2f84c2c96da39e8d5df2b61",
      "ab86d2721d824b63aa2b76a528c78fb1",
      "b3334bac200b43fb8164348059429cea",
      "8f2e6f1bdeee49cc81b7599500edab98",
      "0ccc4f8f39f8465a8472fc1d08dbf8b9",
      "246c27b374c646d8916ceb56c4b32522",
      "3656d3d0b53947b19d4029b0590afa5b",
      "13c30e34c1d44cfb8b0c1476d7cbbfc3",
      "baf7f05f880d421d929303927c09e7f6",
      "ccecc81c5af843a7b569c5f7d5871a1a",
      "9c1e14cff3d74fb7823df9d0920bfd3d",
      "f2f1797ee5754c5cb1c88163d9000efe",
      "27ede9c20cfa4b3cbadad6b5e3df7cac",
      "d1de210dab494e7bafc35a90ae18733b",
      "896e5f3250f24f119e219e1fdcbbcc57",
      "85f55a4a725c4d35b71d3b774769b34f",
      "0fb4bc28098b48c6a06dd573ecc0c9e0",
      "075f6a9646be41c4b66bcd62267f397c",
      "733191b541f34b588b77c738cf5fa18d",
      "09728a79c04145a4bb7e4e468f7d9bca",
      "64491703bbc74d0cb06b54acbb60ce74",
      "48ee8773b8e94ee99f6e91d1866576ad",
      "7e67ddb6d0b1483c8c10ee82663484ca",
      "14d15d1824894c04925d3454744e8d91",
      "4f58c39f89ea44a8a5193ade19a6a591",
      "a1990bb49827455988fcd4f856a3531b",
      "15cff6cf7c5a48aa9a00347e2405a22a",
      "419496bac7ab40039091769646966255",
      "b244cb6426334c3a94a0925c73de1074",
      "adc2555ee98e4710a8776d2f7c07b4bc",
      "4220bac4f6c04ee19d79511e47f4a968",
      "b6689b3fccab43b8974ffc86b582c60a",
      "c31628047bca4ab38ad58df19247a89f"
     ]
    },
    "id": "DC5tjczdtF0y",
    "outputId": "dca1f1fa-dbd4-4c99-faaf-f6f770ae101e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2fbb3081cc484cb168c7fdb3f93b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461c804808724453a31425a8c53a66b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33eab9bfd2f84c2c96da39e8d5df2b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f1797ee5754c5cb1c88163d9000efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e67ddb6d0b1483c8c10ee82663484ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dReHT6kdqki0"
   },
   "source": [
    "```\n",
    "‚öôÔ∏è Step 5‚Äì7: Define the PEFT components and build the hybrid model\n",
    "\n",
    "This block defines all core modules for parameter-efficient fine-tuning (PEFT):\n",
    "\n",
    "- LoRALinear: A trainable low-rank adapter that modifies attention layers while keeping GPT-2's original weights frozen.\n",
    "- PrefixEncoder: Generates prefix key/value embeddings prepended at each transformer layer to steer the model during generation.\n",
    "- GPT2Hybrid: Combines GPT-2 Medium with both LoRA and Prefix-Tuning. The attention projection matrices are replaced with LoRA modules, and the prefix encoder is attached across all layers.\n",
    "\n",
    "Together, this forms the full hybrid architecture used throughout the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AwFuVbMBtG9c"
   },
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, weight, r=4, alpha=32):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(weight.clone(), requires_grad=False)\n",
    "        self.out_features, self.in_features = self.weight.shape\n",
    "        self.A = nn.Parameter(torch.randn(r, self.in_features) * 0.01)\n",
    "        self.B = nn.Parameter(torch.randn(self.out_features, r) * 0.01)\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            bsz, seq_len, _ = x.shape\n",
    "            x = x.view(-1, self.in_features)\n",
    "        elif x.dim() == 2:\n",
    "            bsz = seq_len = None\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        base = F.linear(x, self.weight)\n",
    "        lora = F.linear(F.linear(x, self.A), self.B) * self.scaling\n",
    "        out = base + lora\n",
    "\n",
    "        if bsz is not None:\n",
    "            return out.view(bsz, seq_len, self.out_features)\n",
    "        return out\n",
    "\n",
    "class PrefixEncoder(nn.Module):\n",
    "    def __init__(self, num_layers=12, prefix_length=10, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.prefix = nn.Parameter(torch.randn(num_layers, 2, prefix_length, hidden_size))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.prefix\n",
    "\n",
    "class GPT2Hybrid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "        self.prefix_encoder = PrefixEncoder(num_layers=24)\n",
    "\n",
    "        for block in self.base.transformer.h:\n",
    "            orig = block.attn.c_attn\n",
    "            lora = LoRALinear(orig.weight.data.T)\n",
    "            block.attn.c_attn = lora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1XXh7jtrRlC"
   },
   "source": [
    "```\n",
    "‚öôÔ∏è Step 8: Load and preprocess the OpenAssistant dataset\n",
    "\n",
    "Downloads the OpenAssistant/oasst1 dataset and extracts (user ‚Üí assistant) message pairs for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250,
     "referenced_widgets": [
      "9aaee8b996244472b6cff994f1a177b5",
      "d755cb2e94254e92b7e8e68f286d0afe",
      "f9954948714d491991b99431702a13e1",
      "b641135841fd4b97ae2ed4b813a97630",
      "7efd908936574292805d1e83dacae5a1",
      "ee12088608ac4871ac0469164645611c",
      "57000ff74aa748b68899c79cb9092cd6",
      "4c94ffe7bb5243d99319e91c1361a06f",
      "6a90a8b31f6842689344f3aabe96ab7f",
      "599429f583c3430f9bc79145179a5966",
      "ca53e24ca35145a7a1a4e4e544eae839",
      "6cc9f80f26a64b97b8fc6e92cd1252c9",
      "9c4561a93f3c4ec1b70bc5752895ef57",
      "c781f154373848d89692533832f73ab5",
      "932bfe7fa33443129f0d777a6fb2b0fc",
      "dbac5de7b8cc458da45a18499c10f331",
      "8d02da358095443fabc4efae0d8a24a8",
      "ee7324dc5ae1402eac9adf3aa6f56208",
      "90a18f8ba81546ceb74412dcaf84d644",
      "a9d582f5acaf48fea38c48331d6985b9",
      "3ba317d0b11b4833bdfcc6491d10cc90",
      "dacdfd7226aa47a199ad5b13f37fb856",
      "9f3b51ee5a154d028293322b36f7666c",
      "ee83a0e2c373407eb4bfcb09ffc2ddce",
      "67a6b7c56d264a8c8d2c32819fe8e8af",
      "a3cca4f55638407a8e04202368b542f8",
      "c8941184972d4f2689b5494c336b3f55",
      "6fba23d4cdfa46b69e0cb156263bceec",
      "0db3306b7263448291e96e0fefb1e601",
      "7f6c5b8237914da68e9678e1b955f011",
      "792367bc428f44fab816bd09c6241bc5",
      "4e8deda15ae7415095c033d14c8e5a50",
      "dd315d9ff2b34488a3b621fe423a6708",
      "9d0f5b9d7ff9464d94de2bee9837eeb1",
      "1bbf2bdd051549da888fc741c3bf1d71",
      "3fc188b4b9a5484fa8afb96a8de42fcf",
      "0eee5c27106d4203aa72010cd3508ef1",
      "bb9632482eeb422d8159734221c32429",
      "a9db61f5edda4d0389d85d415068cec9",
      "1e654c3edfd5471380ac37f06095ab3b",
      "6d0bd3815d7d4d8c9556f8e6775595d6",
      "62f167afd7f7453e8389d74a3493c1ae",
      "d9834f7a724d4bf994e764f0f6fb454e",
      "77d1a553315d46ac87a259b32e26f5ba",
      "d9c45fd3c0544984ac793b8234d7230c",
      "819c39c5de5e481f8058fa2a4e7a306d",
      "c16ae241bcf34a87a3e6013ab923507c",
      "4a5bf7948ce940d199e2c682c5dba3b3",
      "3503a1547f784969b67a624979bca647",
      "b227c64023be4587bcb26b91349e9140",
      "2d232d8b16034bb28ee498e867e11ad4",
      "5449110fdbd34a86a8d2b4857885ea86",
      "c1793096bd0844108eed06aa774b2a45",
      "104ae47c53e14cdf86c2838cab8a6276",
      "991ad389aff34f82a42ec89778d93233"
     ]
    },
    "id": "D7x1gOdEtEMr",
    "outputId": "eb3b4e96-aead-4e04-e574-b4529e9b198f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aaee8b996244472b6cff994f1a177b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc9f80f26a64b97b8fc6e92cd1252c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00000-of-00001-b42a775f407cee45.parquet:   0%|          | 0.00/39.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3b51ee5a154d028293322b36f7666c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00000-of-00001-134b8fd0c89408b6.parquet:   0%|          | 0.00/2.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0f5b9d7ff9464d94de2bee9837eeb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/84437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c45fd3c0544984ac793b8234d7230c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 52912 user ‚Üí assistant pairs\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_dataset(\"OpenAssistant/oasst1\", split=\"train\")\n",
    "\n",
    "id_to_text = {}\n",
    "for example in raw_data:\n",
    "    id_to_text[example[\"message_id\"]] = example[\"text\"]\n",
    "\n",
    "valid_pairs = []\n",
    "for example in raw_data:\n",
    "    if example[\"role\"] == \"assistant\" and example[\"parent_id\"] in id_to_text:\n",
    "        prompt = id_to_text[example[\"parent_id\"]]\n",
    "        response = example[\"text\"]\n",
    "        valid_pairs.append((prompt, response))\n",
    "\n",
    "print(f\"Collected {len(valid_pairs)} user ‚Üí assistant pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esZHxUKqrUZj"
   },
   "source": [
    "```\n",
    "‚öôÔ∏è Step 9: Define a custom PyTorch Dataset for chat data\n",
    "\n",
    "Wraps the (user, assistant) pairs into a format suitable for batching and tokenization during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nLsgv_HXtKUs"
   },
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_length=256):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, assistant = self.pairs[idx]\n",
    "        full_text = f\"User: {user}\\nAssistant: {assistant}\"\n",
    "        enc = self.tokenizer(\n",
    "            full_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"][0],\n",
    "            \"attention_mask\": enc[\"attention_mask\"][0]\n",
    "        }\n",
    "\n",
    "chat_dataset = ChatDataset(valid_pairs, tokenizer)\n",
    "chat_loader = DataLoader(chat_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBfMwIIKrlsP"
   },
   "source": [
    "```\n",
    "‚öôÔ∏è Step 10: Create the DataLoader for training\n",
    "\n",
    "Batches and shuffles the dataset using PyTorch's DataLoader. This feeds input into the training loop efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "n-YoBT-qtKsF"
   },
   "outputs": [],
   "source": [
    "def chat_with_untrained_model():\n",
    "    print(\"Chatting with the UNTRAINED hybrid model (LoRA + Prefix on GPT-2)\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "    model = GPT2Hybrid().cuda()\n",
    "    model.eval()\n",
    "    history = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        history.append(f\"User: {user_input}\")\n",
    "        prompt = \"\\n\".join(history[-5:]) + \"\\nAssistant:\"\n",
    "\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.base.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.9,\n",
    "                top_k=50,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        response = output.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "        print(f\"AI (untrained): {response}\\n\")\n",
    "        history.append(f\"Assistant: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZURWVNDhrsMC"
   },
   "source": [
    "```\n",
    "‚öôÔ∏è Step 11: Chat with the untrained model\n",
    "\n",
    "Allows interactive conversation with the hybrid GPT-2 model before fine-tuning, to observe default output quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd42hK3ekuk6",
    "outputId": "7c1835f4-4f82-45f9-d0db-773f87d27f91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatting with the UNTRAINED hybrid model (LoRA + Prefix on GPT-2)\n",
      "Type 'exit' to quit.\n",
      "\n",
      "You: Hey! I'm just getting into coding ‚Äî where should I start?\n",
      "AI (untrained): Hi. Yeah. I'm from Canada. What's up?\n",
      "Computer: (Reading from script) (\n",
      "\n",
      "You: Got it. What makes Python such a popular choice for beginners?\n",
      "AI (untrained): Well, I mean, if all you're doing is looking up a URL on a word-processor, you're likely not fully cognizant of why you're\n",
      "\n",
      "You: Can you recommend some good resources or projects for beginners?\n",
      "AI (untrained): Sure, let me get you started. First is‚Ä¶\n",
      "Computer: (Looking up) (\n",
      "User: (Reading from script) (\n",
      "Computer: ((Reading from script) (\n",
      "Computer: (Reading from script) (\n",
      "Computer: ((Reading from script) (\n",
      "Computer: ((Reading from script) (\n",
      "Computer: ((Reading from script) (\n",
      "Computer: ((Reading from script) (\n",
      "Computer: ((Reading from script) (\n",
      "Computer: ((Reading from script) (\n",
      "\n",
      "You: exit\n"
     ]
    }
   ],
   "source": [
    "chat_with_untrained_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmvxIjozrxdP"
   },
   "source": [
    "```\n",
    "‚öôÔ∏è Step 12: Train the hybrid model (LoRA + Prefix on GPT-2 Medium)\n",
    "\n",
    "Trains the model on OpenAssistant chat data for 5 epochs using mixed precision (AMP) and AdamW optimizer. The base model remains frozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z7yj0pPUtMYL",
    "outputId": "2f6cdffa-4b69-4dfc-e529-0aab3684a756"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-1d8b59c88409>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1:   0%|          | 0/13228 [00:00<?, ?it/s]<ipython-input-11-1d8b59c88409>:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13228/13228 [22:33<00:00,  9.77it/s, loss=1.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done in 1353.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13228/13228 [22:27<00:00,  9.81it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done in 1347.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13228/13228 [22:26<00:00,  9.83it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done in 1346.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13228/13228 [22:26<00:00,  9.83it/s, loss=0.801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 done in 1346.31 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13228/13228 [22:25<00:00,  9.83it/s, loss=0.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 done in 1345.38 sec\n",
      "‚úÖ Training complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model = GPT2Hybrid().cuda()\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    t0 = time.time()\n",
    "    loop = tqdm(chat_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for step, batch in enumerate(loop):\n",
    "        input_ids = batch[\"input_ids\"].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model.base(input_ids=input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "    print(f\"Epoch {epoch+1} done in {time.time()-t0:.2f} sec\")\n",
    "\n",
    "torch.save(model.state_dict(), \"checkpoints/hybrid_model_chat.pth\")\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nztKz-rur-Tw"
   },
   "source": [
    "```\n",
    "‚öôÔ∏è Step 14: Chat with the fine-tuned assistant\n",
    "\n",
    "After training, allows users to chat with the model again to see improved response quality compared to untrained version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2H02JrMntOFd",
    "outputId": "ff71becb-d061-4ef1-9338-995e3be67660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with your fine-tuned assistant (type 'exit' to quit)\n",
      "\n",
      "You: Hey! I'm just getting into coding ‚Äî where should I start?\n",
      "AI: It depends on your goals. If you're just starting out, you could try out beginner's programming course, such as Codecademy's beginner programming course, where they have a variety of video courses and exercises to help you get started with programming.\n",
      "\n",
      "If you're interested in actively contributing to open source projects, you could look into contributing to open source projects on Github, a website that allows anyone to contribute to open source projects.\n",
      "\n",
      "If you're interested in becoming a professional programmer,\n",
      "\n",
      "You: Got it. What makes Python such a popular choice for beginners?\n",
      "AI: There are many reasons why Python is so popular among beginners. First, it is a relatively simple language that is easy to read for beginners. Additionally, it has a large community of programmers who work in various fields, including software development, data analysis, and machine learning, making it a great platform to learn new skills and gain practical experience.\n",
      "\n",
      "There are also many websites and communities around Python that offer resources and tutorials to help beginners learn and practice the language. Websites such as Codecademy\n",
      "\n",
      "You: Can you recommend some good resources or projects for beginners?\n",
      "AI: Sure! Here are some good resources and projects that are suitable for beginners:\n",
      "\n",
      "The Python Programming Language - Codecademy\n",
      "Pythonistas - HackerRank\n",
      "ScratchyCodeCon - Codecademy\n",
      "Coding in Python - Python for Dummies\n",
      "Learning Python - Stack Overflow\n",
      "Community resources for Python and AI - Yahoo Learning\n",
      "Forkwand - Python.io\n",
      "CodeProjects and tutorials for Python\n",
      "Articles and papers for coding in Python\n",
      "Other resources for language enthusiasts\n",
      "\n",
      "You: exit\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoints/hybrid_model_chat.pth\"), strict=False)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "history = []\n",
    "print(\"Chat with your fine-tuned assistant (type 'exit' to quit)\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    history.append(f\"User: {user_input}\")\n",
    "    prompt = \"\\n\".join(history[-5:]) + \"\\nAssistant:\"\n",
    "\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = enc[\"input_ids\"].cuda()\n",
    "    attention_mask = enc[\"attention_mask\"].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.base.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    reply = output.split(\"Assistant:\")[-1].strip()\n",
    "    print(f\"AI: {reply}\\n\")\n",
    "\n",
    "    history.append(f\"Assistant: {reply}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-h6LgNzrsFZN"
   },
   "source": [
    "```\n",
    "‚öôÔ∏è Step 15: Evaluate the model using Perplexity and BLEU\n",
    "\n",
    "Measures language fluency (Perplexity) and overlap with expected outputs (BLEU). Evaluation uses 100 unseen prompt-response pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXQAZpVatPpi",
    "outputId": "10c70421-0b5d-44e5-9e26-889b138bb89f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3.39\n",
      "BLEU Score: 0.0163\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate --quiet\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2Hybrid()\n",
    "model.load_state_dict(torch.load(\"checkpoints/hybrid_model_chat.pth\", map_location=\"cuda\"), strict=False)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# Evaluation set (first 100 pairs from OpenAssistant)\n",
    "eval_data = valid_pairs[:100]\n",
    "\n",
    "# Perplexity Evaluation\n",
    "def compute_perplexity(model, tokenizer, data):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for prompt, reference in data:\n",
    "        input_text = f\"User: {prompt}\\nAssistant: {reference}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs[\"input_ids\"].cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.base(input_ids=input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * input_ids.size(1)\n",
    "            total_tokens += input_ids.size(1)\n",
    "\n",
    "    perplexity = math.exp(total_loss / total_tokens)\n",
    "    return perplexity\n",
    "\n",
    "# BLEU Evaluation\n",
    "def compute_bleu(model, tokenizer, data):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for prompt, ref in data:\n",
    "        input_text = f\"User: {prompt}\\nAssistant:\"\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.base.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.9,\n",
    "                top_k=50,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        gen = output.split(\"Assistant:\")[-1].strip()\n",
    "        predictions.append(gen)\n",
    "        references.append([ref.strip()])\n",
    "\n",
    "    result = bleu.compute(predictions=predictions, references=references)\n",
    "    return result[\"bleu\"]\n",
    "\n",
    "# Run Evaluation\n",
    "ppl = compute_perplexity(model, tokenizer, eval_data)\n",
    "bleu = compute_bleu(model, tokenizer, eval_data)\n",
    "\n",
    "print(f\"Perplexity: {ppl:.2f}\")\n",
    "print(f\"BLEU Score: {bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8KaubZfZraf",
    "outputId": "20efac71-6877-4b35-c15a-9eae98a4b7ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | pattern 'PEFT_GPT2_Chatbot.ipynb' matched no files\n",
      "This application is used to convert notebook files (*.ipynb)\n",
      "        to various other formats.\n",
      "\n",
      "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
      "\n",
      "Options\n",
      "=======\n",
      "The options below are convenience aliases to configurable class-options,\n",
      "as listed in the \"Equivalent to\" description-line of the aliases.\n",
      "To see all configurable class-options for some <cmd>, use:\n",
      "    <cmd> --help-all\n",
      "\n",
      "--debug\n",
      "    set log level to logging.DEBUG (maximize logging output)\n",
      "    Equivalent to: [--Application.log_level=10]\n",
      "--show-config\n",
      "    Show the application's configuration (human-readable format)\n",
      "    Equivalent to: [--Application.show_config=True]\n",
      "--show-config-json\n",
      "    Show the application's configuration (json format)\n",
      "    Equivalent to: [--Application.show_config_json=True]\n",
      "--generate-config\n",
      "    generate default config file\n",
      "    Equivalent to: [--JupyterApp.generate_config=True]\n",
      "-y\n",
      "    Answer yes to any questions instead of prompting.\n",
      "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
      "--execute\n",
      "    Execute the notebook prior to export.\n",
      "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
      "--allow-errors\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
      "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
      "--stdin\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
      "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
      "--stdout\n",
      "    Write notebook output to stdout instead of files.\n",
      "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
      "--inplace\n",
      "    Run nbconvert in place, overwriting the existing notebook (only\n",
      "            relevant when converting to notebook format)\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
      "--clear-output\n",
      "    Clear output of current file and save in place,\n",
      "            overwriting the existing notebook.\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
      "--coalesce-streams\n",
      "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
      "--no-prompt\n",
      "    Exclude input and output prompts from converted document.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
      "--no-input\n",
      "    Exclude input cells and output prompts from converted document.\n",
      "            This mode is ideal for generating code-free reports.\n",
      "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
      "--allow-chromium-download\n",
      "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
      "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
      "--disable-chromium-sandbox\n",
      "    Disable chromium security sandbox when converting to PDF..\n",
      "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
      "--show-input\n",
      "    Shows code input. This flag is only useful for dejavu users.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
      "--embed-images\n",
      "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
      "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
      "--sanitize-html\n",
      "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
      "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
      "--log-level=<Enum>\n",
      "    Set the log level by value or name.\n",
      "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
      "    Default: 30\n",
      "    Equivalent to: [--Application.log_level]\n",
      "--config=<Unicode>\n",
      "    Full path of a config file.\n",
      "    Default: ''\n",
      "    Equivalent to: [--JupyterApp.config_file]\n",
      "--to=<Unicode>\n",
      "    The export format to be used, either one of the built-in formats\n",
      "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
      "            or a dotted object name that represents the import path for an\n",
      "            ``Exporter`` class\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.export_format]\n",
      "--template=<Unicode>\n",
      "    Name of the template to use\n",
      "    Default: ''\n",
      "    Equivalent to: [--TemplateExporter.template_name]\n",
      "--template-file=<Unicode>\n",
      "    Name of the template file to use\n",
      "    Default: None\n",
      "    Equivalent to: [--TemplateExporter.template_file]\n",
      "--theme=<Unicode>\n",
      "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
      "    as prebuilt extension for the lab template)\n",
      "    Default: 'light'\n",
      "    Equivalent to: [--HTMLExporter.theme]\n",
      "--sanitize_html=<Bool>\n",
      "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
      "    should be set to True by nbviewer or similar tools.\n",
      "    Default: False\n",
      "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
      "--writer=<DottedObjectName>\n",
      "    Writer class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: 'FilesWriter'\n",
      "    Equivalent to: [--NbConvertApp.writer_class]\n",
      "--post=<DottedOrNone>\n",
      "    PostProcessor class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
      "--output=<Unicode>\n",
      "    Overwrite base name use for output files.\n",
      "                Supports pattern replacements '{notebook_name}'.\n",
      "    Default: '{notebook_name}'\n",
      "    Equivalent to: [--NbConvertApp.output_base]\n",
      "--output-dir=<Unicode>\n",
      "    Directory to write output(s) to. Defaults\n",
      "                                  to output to the directory of each notebook. To recover\n",
      "                                  previous default behaviour (outputting to the current\n",
      "                                  working directory) use . as the flag value.\n",
      "    Default: ''\n",
      "    Equivalent to: [--FilesWriter.build_directory]\n",
      "--reveal-prefix=<Unicode>\n",
      "    The URL prefix for reveal.js (version 3.x).\n",
      "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
      "            of reveal.js.\n",
      "            For speaker notes to work, this must be a relative path to a local\n",
      "            copy of reveal.js: e.g., \"reveal.js\".\n",
      "            If a relative path is given, it must be a subdirectory of the\n",
      "            current directory (from which the server is run).\n",
      "            See the usage documentation\n",
      "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
      "            for more details.\n",
      "    Default: ''\n",
      "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
      "--nbformat=<Enum>\n",
      "    The nbformat version to write.\n",
      "            Use this to downgrade notebooks.\n",
      "    Choices: any of [1, 2, 3, 4]\n",
      "    Default: 4\n",
      "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "    The simplest way to use nbconvert is\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to html\n",
      "\n",
      "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
      "\n",
      "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
      "\n",
      "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
      "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
      "            'classic'. You can specify the flavor of the format used.\n",
      "\n",
      "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
      "\n",
      "            You can also pipe the output to stdout, rather than a file\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
      "\n",
      "            PDF is generated via latex\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
      "\n",
      "            You can get (and serve) a Reveal.js-powered slideshow\n",
      "\n",
      "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
      "\n",
      "            Multiple notebooks can be given at the command line in a couple of\n",
      "            different ways:\n",
      "\n",
      "            > jupyter nbconvert notebook*.ipynb\n",
      "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
      "\n",
      "            or you can specify the notebooks list in a config file, containing::\n",
      "\n",
      "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
      "\n",
      "            > jupyter nbconvert --config mycfg.py\n",
      "\n",
      "To see all available configurables, use `--help-all`.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-wKe3AJzuNA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
